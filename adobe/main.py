import os
import re
import json
import pdfplumber
from collections import Counter
import unicodedata

INPUT_DIR = "app/input"
OUTPUT_DIR = "app/output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Multilingual noise patterns
MULTILINGUAL_NOISE_PATTERNS = {
    'universal': [
        r'copyright|Â©|Â®|â„¢',
        r'page \d+|é¡µ\s*\d+|ãƒšãƒ¼ã‚¸\s*\d+|í˜ì´ì§€\s*\d+|pÃ¡gina\s*\d+|seite\s*\d+',
        r'version|à¤¸à¤‚à¤¸à¥à¤•à¤°à¤£|ç‰ˆæœ¬|ãƒãƒ¼ã‚¸ãƒ§ãƒ³|ë²„ì „|versiÃ³n|version',
        r'www\.|http|\.com|\.org',
        r'[.\-_]{4,}',  # Dot leaders
        r'^\d{1,2}:\d{2}|^\d{1,2}/\d{1,2}/\d{2,4}',  # Times/dates
        r'^[^\w\s]*$',  # Only punctuation
    ],
    'english': [
        r'all rights reserved|confidential|internal use|draft',
        r'table of contents|index|appendix',
    ],
    'hindi': [
        r'à¤¸à¤­à¥€ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤|à¤—à¥‹à¤ªà¤¨à¥€à¤¯|à¤†à¤‚à¤¤à¤°à¤¿à¤• à¤‰à¤ªà¤¯à¥‹à¤—|à¤®à¤¸à¥Œà¤¦à¤¾',
        r'à¤µà¤¿à¤·à¤¯ à¤¸à¥‚à¤šà¥€|à¤…à¤¨à¥à¤•à¥à¤°à¤®à¤£à¤¿à¤•à¤¾|à¤ªà¤°à¤¿à¤¶à¤¿à¤·à¥à¤Ÿ',
    ],
    'chinese': [
        r'ç‰ˆæƒæ‰€æœ‰|ä¿å¯†|å†…éƒ¨ä½¿ç”¨|è‰ç¨¿',
        r'ç›®å½•|ç´¢å¼•|é™„å½•',
    ],
    'japanese': [
        r'è‘—ä½œæ¨©|æ©Ÿå¯†|å†…éƒ¨ä½¿ç”¨|ä¸‹æ›¸ã',
        r'ç›®æ¬¡|ç´¢å¼•|ä»˜éŒ²',
    ],
    'korean': [
        r'ì €ì‘ê¶Œ|ê¸°ë°€|ë‚´ë¶€ ì‚¬ìš©|ì´ˆì•ˆ',
        r'ëª©ì°¨|ìƒ‰ì¸|ë¶€ë¡',
    ],
    'spanish': [
        r'derechos reservados|confidencial|uso interno|borrador',
        r'Ã­ndice|tabla de contenidos|apÃ©ndice',
    ],
    'french': [
        r'droits rÃ©servÃ©s|confidentiel|usage interne|brouillon',
        r'table des matiÃ¨res|index|annexe',
    ],
    'german': [
        r'alle rechte vorbehalten|vertraulich|interne verwendung|entwurf',
        r'inhaltsverzeichnis|index|anhang',
    ]
}

# Multilingual heading indicators
MULTILINGUAL_HEADING_PATTERNS = {
    'numbered_sections': [
        r'^\d+(\.\d+)*\.?\s+',  # Universal numbering
        r'^ç¬¬\d+ç« |^ç¬¬\d+èŠ‚',  # Chinese chapters/sections
        r'^ç¬¬\d+ç« |^ç¬¬\d+ç¯€',  # Traditional Chinese
        r'^\d+ì¥|^\d+ì ˆ',      # Korean chapters/sections
        r'^ç¬¬\d+ç« ',           # Japanese chapters
        r'^à¤…à¤§à¥à¤¯à¤¾à¤¯\s*\d+|^à¤–à¤‚à¤¡\s*\d+',  # Hindi chapters/sections
        r'^capÃ­tulo\s*\d+|^secciÃ³n\s*\d+',  # Spanish
        r'^chapitre\s*\d+|^section\s*\d+',  # French
        r'^kapitel\s*\d+|^abschnitt\s*\d+', # German
    ],
    'appendix_patterns': [
        r'^appendix\s+[a-z]',     # English
        r'^anexo\s+[a-z]',        # Spanish
        r'^annexe\s+[a-z]',       # French
        r'^anhang\s+[a-z]',       # German
        r'^à¤ªà¤°à¤¿à¤¶à¤¿à¤·à¥à¤Ÿ\s*[a-z]',      # Hindi
        r'^é™„å½•\s*[a-z]',          # Chinese
        r'^ä»˜éŒ²\s*[a-z]',          # Japanese
        r'^ë¶€ë¡\s*[a-z]',          # Korean
    ]
}

# Common instruction words in different languages
INSTRUCTION_WORDS = {
    'english': ['required', 'please', 'visit', 'fill', 'complete', 'enter', 'select'],
    'hindi': ['à¤†à¤µà¤¶à¥à¤¯à¤•', 'à¤•à¥ƒà¤ªà¤¯à¤¾', 'à¤­à¤°à¥‡à¤‚', 'à¤ªà¥‚à¤°à¤¾', 'à¤¦à¤°à¥à¤œ', 'à¤šà¥à¤¨à¥‡à¤‚'],
    'chinese': ['å¿…éœ€', 'è¯·', 'å¡«å†™', 'å®Œæˆ', 'è¾“å…¥', 'é€‰æ‹©'],
    'japanese': ['å¿…è¦', 'ã—ã¦ãã ã•ã„', 'è¨˜å…¥', 'å®Œäº†', 'å…¥åŠ›', 'é¸æŠ'],
    'korean': ['í•„ìˆ˜', 'ì œë°œ', 'ì±„ìš°ë‹¤', 'ì™„ë£Œ', 'ì…ë ¥', 'ì„ íƒ'],
    'spanish': ['requerido', 'por favor', 'llenar', 'completar', 'entrar', 'seleccionar'],
    'french': ['requis', 's\'il vous plaÃ®t', 'remplir', 'complÃ©ter', 'entrer', 'sÃ©lectionner'],
    'german': ['erforderlich', 'bitte', 'ausfÃ¼llen', 'vervollstÃ¤ndigen', 'eingeben', 'auswÃ¤hlen']
}

def detect_script_type(text):
    """Detect the primary script/language family of text"""
    if not text:
        return 'latin'
    
    scripts = {
        'chinese': 0,
        'japanese': 0,
        'korean': 0,
        'devanagari': 0,  # Hindi
        'latin': 0,
        'cyrillic': 0
    }
    
    for char in text:
        if '\u4e00' <= char <= '\u9fff':  # CJK Unified Ideographs
            scripts['chinese'] += 1
        elif '\u3040' <= char <= '\u309f' or '\u30a0' <= char <= '\u30ff':  # Hiragana/Katakana
            scripts['japanese'] += 1
        elif '\uac00' <= char <= '\ud7af':  # Hangul
            scripts['korean'] += 1
        elif '\u0900' <= char <= '\u097f':  # Devanagari
            scripts['devanagari'] += 1
        elif '\u0400' <= char <= '\u04ff':  # Cyrillic
            scripts['cyrillic'] += 1
        elif char.isalpha():
            scripts['latin'] += 1
    
    return max(scripts, key=scripts.get)

def is_multilingual_noise(line, repeated_lines, line_context):
    """Enhanced multilingual noise detection"""
    line_lower = line.lower().strip()
    
    # Skip if repeated across pages (headers/footers)
    if line.strip() in repeated_lines:
        return True
    
    # Basic noise patterns
    if not line_lower or len(line_lower) < 2:
        return True
    
    # Check against all multilingual noise patterns
    for lang_patterns in MULTILINGUAL_NOISE_PATTERNS.values():
        for pattern in lang_patterns:
            if re.search(pattern, line_lower, re.IGNORECASE):
                return True
    
    # Skip if part of address/form block
    if line_context.get('short_lines_nearby', 0) >= 4:
        return True
    
    # Skip pure numbers or symbols
    if re.match(r'^[\d\s\-_.()]+$', line.strip()):
        return True
    
    return False

def is_multilingual_heading_candidate(line, context, doc_stats):
    """Enhanced multilingual heading detection"""
    words = line.split()
    line_clean = line.strip()
    script_type = detect_script_type(line)
    
    # Adjust length constraints based on script
    if script_type in ['chinese', 'japanese']:
        max_chars = 50  # CJK characters are denser
        max_words = 20
    else:
        max_chars = 120
        max_words = 15
    
    # Length constraints
    if len(words) > max_words or len(line) > max_chars:
        return False
    
    # Skip if surrounded by many short lines (forms/addresses)
    if context.get('short_lines_nearby', 0) >= 4:
        return False
    
    # Strong positive signals
    
    # 1. Numbered sections (universal)
    for pattern in MULTILINGUAL_HEADING_PATTERNS['numbered_sections']:
        if re.match(pattern, line, re.IGNORECASE):
            return True
    
    # 2. Appendix patterns (multilingual)
    for pattern in MULTILINGUAL_HEADING_PATTERNS['appendix_patterns']:
        if re.match(pattern, line, re.IGNORECASE):
            return True
    
    # 3. All caps headings (but exclude instructions)
    if line.isupper() and 2 <= len(words) <= 8:
        # Check against instruction words in all languages
        is_instruction = False
        for lang_words in INSTRUCTION_WORDS.values():
            if any(word.lower() in line.lower() for word in lang_words):
                is_instruction = True
                break
        
        if not is_instruction:
            # Exclude address patterns (more universal)
            if not re.match(r'^\d+\s+[A-Z\s]+$|^[A-Z\s]+,\s*[A-Z]{2}', line):
                return True
    
    # 4. Title case headings (mainly for Latin scripts)
    if script_type == 'latin' and line.istitle() and 3 <= len(words) <= 10:
        if not line.endswith(':'):
            return True
    
    # 5. High uppercase ratio (for Latin scripts)
    if script_type == 'latin':
        uppercase_ratio = sum(c.isupper() for c in line) / max(1, len(line))
        if 0.6 <= uppercase_ratio < 1.0 and len(words) <= 10:
            return True
    
    # 6. Colon-ended section headers (universal)
    if line.endswith(':') and 2 <= len(words) <= 8:
        return True
    
    # 7. CJK specific patterns
    if script_type in ['chinese', 'japanese']:
        # Check for common CJK section markers
        if re.search(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', line):
            return True
        # Bold or emphasized text patterns
        if len(line) <= 30 and not line.endswith('ã€‚'):  # Not ending with period
            return True
    
    # 8. Korean specific patterns
    if script_type == 'korean':
        # Korean section markers
        if re.search(r'[ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜]\.', line):
            return True
    
    # 9. Hindi/Devanagari specific patterns
    if script_type == 'devanagari':
        # Hindi numbering and section markers
        if re.search(r'[à¥§à¥¨à¥©à¥ªà¥«à¥¬à¥­à¥®à¥¯à¥¦]|à¤…à¤§à¥à¤¯à¤¾à¤¯|à¤­à¤¾à¤—|à¤–à¤‚à¤¡', line):
            return True
    
    return False

def get_multilingual_heading_level(text):
    """Determine heading level with multilingual support"""
    text = text.strip()
    script_type = detect_script_type(text)
    
    # Multi-level numbering (universal)
    if re.match(r'^\d+(\.\d+){3,}', text):
        return "H4"
    elif re.match(r'^\d+(\.\d+){2}', text):
        return "H3"
    elif re.match(r'^\d+\.\d+', text):
        return "H2"
    elif re.match(r'^\d+\.?\s', text):
        return "H1"
    
    # Chinese/Japanese chapter patterns
    elif re.match(r'^ç¬¬\d+ç« ', text):
        return "H1"
    elif re.match(r'^ç¬¬\d+èŠ‚|^ç¬¬\d+ç¯€', text):
        return "H2"
    
    # Korean patterns
    elif re.match(r'^\d+ì¥', text):
        return "H1"
    elif re.match(r'^\d+ì ˆ', text):
        return "H2"
    
    # Hindi patterns
    elif re.match(r'^à¤…à¤§à¥à¤¯à¤¾à¤¯\s*\d+', text):
        return "H1"
    elif re.match(r'^à¤–à¤‚à¤¡\s*\d+|^à¤­à¤¾à¤—\s*\d+', text):
        return "H2"
    
    # Appendix patterns (multilingual)
    elif any(re.match(pattern, text, re.IGNORECASE) 
             for pattern in MULTILINGUAL_HEADING_PATTERNS['appendix_patterns']):
        return "H2"
    
    # Colon-ended headers
    elif text.endswith(':') and len(text.split()) <= 5:
        return "H3"
    
    # CJK specific levels
    elif script_type in ['chinese', 'japanese']:
        if re.search(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€', text):
            return "H2"
        elif re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', text):
            return "H3"
    
    return "H1"

def clean_multilingual_heading(text):
    """Clean heading text with multilingual support"""
    text = text.strip()
    
    # Remove trailing dots and page numbers (universal)
    text = re.sub(r'[.\-_]{3,}$', '', text)
    text = re.sub(r'\s+\d+$', '', text)
    text = re.sub(r'\s{2,}', ' ', text)
    
    # Remove common trailing punctuation
    text = re.sub(r'[.ã€‚ï¼]{1,2}$', '', text)
    
    # Normalize Unicode (important for multilingual text)
    text = unicodedata.normalize('NFKC', text)
    
    return text.strip()

def extract_multilingual_title(first_page_lines, repeated_lines):
    """Extract document title with multilingual support"""
    for line in first_page_lines[:10]:  # Check more lines for multilingual docs
        line_clean = line.strip()
        script_type = detect_script_type(line_clean)
        
        # Adjust constraints based on script
        if script_type in ['chinese', 'japanese']:
            min_len, max_len, max_words = 3, 80, 25
        else:
            min_len, max_len, max_words = 5, 100, 15
        
        if (min_len < len(line_clean) < max_len and 
            line_clean not in repeated_lines and
            len(line_clean.split()) <= max_words and
            not is_multilingual_noise(line_clean, repeated_lines, {})):
            return clean_multilingual_heading(line_clean)
    return ""

def extract_outline(pdf_path):
    """Main extraction function with multilingual support"""
    with pdfplumber.open(pdf_path) as pdf:
        all_lines = []
        page_texts = []
        
        # Collect all text
        for page in pdf.pages:
            text = page.extract_text() or ""
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            page_texts.append(lines)
            all_lines.extend(lines)
        
        if not all_lines:
            return {"title": "", "outline": []}
        
        # Calculate document statistics
        doc_stats = {
            'total_lines': len(all_lines),
            'avg_line_length': sum(len(line.split()) for line in all_lines) / len(all_lines),
            'primary_script': detect_script_type(' '.join(all_lines[:50]))  # Detect from first 50 lines
        }
        
        # Find repeated lines (headers/footers)
        line_counts = Counter(all_lines)
        repeated_threshold = max(2, len(pdf.pages) // 3)
        repeated_lines = {line for line, count in line_counts.items() 
                         if count >= repeated_threshold}
        
        # Extract title
        title = extract_multilingual_title(page_texts[0] if page_texts else [], repeated_lines)
        
        # For documents with very few unique lines, likely forms
        unique_ratio = len(set(all_lines)) / len(all_lines)
        if unique_ratio < 0.3 and len(all_lines) < 50:
            return {"title": title, "outline": []}
        
        # Extract headings
        outline = []
        seen_headings = set()
        
        for page_idx, lines in enumerate(page_texts):
            for line_idx, line in enumerate(lines):
                if not line.strip():
                    continue
                
                # Analyze context
                context = analyze_line_context(lines, line_idx)
                
                # Skip noise
                if is_multilingual_noise(line, repeated_lines, context):
                    continue
                
                # Check if it's a heading candidate
                if is_multilingual_heading_candidate(line, context, doc_stats):
                    clean_text = clean_multilingual_heading(line)
                    
                    # Skip duplicates and title
                    if (clean_text and 
                        clean_text.lower() != title.lower() and 
                        clean_text.lower() not in seen_headings):
                        
                        level = get_multilingual_heading_level(clean_text)
                        
                        outline.append({
                            "level": level,
                            "text": clean_text,
                            "page": page_idx,  # Zero-based indexing
                            "script_type": detect_script_type(clean_text)
                        })
                        
                        seen_headings.add(clean_text.lower())
        
        return {
            "title": title, 
            "outline": outline,
            "document_script": doc_stats['primary_script']
        }

def analyze_line_context(lines, current_idx):
    """Analyze context around current line (unchanged)"""
    context = {'short_lines_nearby': 0}
    
    # Count short lines in vicinity (Â±3 lines)
    start = max(0, current_idx - 3)
    end = min(len(lines), current_idx + 4)
    
    for i in range(start, end):
        if i != current_idx and lines[i].strip():
            words = lines[i].split()
            if len(words) <= 6:
                context['short_lines_nearby'] += 1
    
    return context

# Process all PDFs
if __name__ == "__main__":
    print("ğŸŒ Multilingual PDF Outline Extractor")
    print("Supports: English, Hindi, Chinese, Japanese, Korean, Spanish, French, German")
    print("-" * 70)
    
    processed_count = 0
    for filename in os.listdir(INPUT_DIR):
        if filename.lower().endswith('.pdf'):
            pdf_path = os.path.join(INPUT_DIR, filename)
            try:
                result = extract_outline(pdf_path)
                output_path = os.path.join(OUTPUT_DIR, filename.replace('.pdf', '.json'))
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                
                script_info = f"({result.get('document_script', 'unknown')} script)"
                outline_count = len(result.get('outline', []))
                print(f"âœ… {filename} {script_info} - {outline_count} headings extracted")
                processed_count += 1
                
            except Exception as e:
                print(f"âŒ Error processing {filename}: {str(e)}")
    
    print(f"\nğŸ‰ Processing complete! {processed_count} files processed.")
